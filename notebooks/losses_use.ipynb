{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8099132,"sourceType":"datasetVersion","datasetId":4782590},{"sourceId":8099504,"sourceType":"datasetVersion","datasetId":4782886},{"sourceId":8099681,"sourceType":"datasetVersion","datasetId":4783015},{"sourceId":3973359,"sourceType":"datasetVersion","datasetId":2358085},{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone -b losses_marina https://github.com/MarynaHorbach/GenModelsProject.git","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:01:47.782071Z","iopub.execute_input":"2024-04-12T13:01:47.782341Z","iopub.status.idle":"2024-04-12T13:01:49.523582Z","shell.execute_reply.started":"2024-04-12T13:01:47.782317Z","shell.execute_reply":"2024-04-12T13:01:49.522527Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'GenModelsProject'...\nremote: Enumerating objects: 102, done.\u001b[K\nremote: Counting objects: 100% (102/102), done.\u001b[K\nremote: Compressing objects: 100% (82/82), done.\u001b[K\nremote: Total 102 (delta 33), reused 85 (delta 19), pack-reused 0\u001b[K\nReceiving objects: 100% (102/102), 2.47 MiB | 14.40 MiB/s, done.\nResolving deltas: 100% (33/33), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd GenModelsProject\n!git status","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:01:49.525817Z","iopub.execute_input":"2024-04-12T13:01:49.526091Z","iopub.status.idle":"2024-04-12T13:01:50.470230Z","shell.execute_reply.started":"2024-04-12T13:01:49.526066Z","shell.execute_reply":"2024-04-12T13:01:50.469369Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/GenModelsProject\nOn branch losses_marina\nYour branch is up to date with 'origin/losses_marina'.\n\nnothing to commit, working tree clean\n","output_type":"stream"}]},{"cell_type":"code","source":"!git branch","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:01:50.471835Z","iopub.execute_input":"2024-04-12T13:01:50.472257Z","iopub.status.idle":"2024-04-12T13:01:51.441003Z","shell.execute_reply.started":"2024-04-12T13:01:50.472221Z","shell.execute_reply":"2024-04-12T13:01:51.440111Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"* \u001b[32mlosses_marina\u001b[m\n","output_type":"stream"}]},{"cell_type":"code","source":" !ls","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:01:51.443773Z","iopub.execute_input":"2024-04-12T13:01:51.444051Z","iopub.status.idle":"2024-04-12T13:01:52.394140Z","shell.execute_reply.started":"2024-04-12T13:01:51.444025Z","shell.execute_reply":"2024-04-12T13:01:52.392846Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"artifacts  losses  notebooks\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Losses\n\n0. Для начала нам нужно собрать все нужные модели, а именно:\n\n- Discriminator (основной, от StyleGAN)\n\n- ArcFace (для IdentityPreservationLoss)\n\n- LandmarkEstimator (для LandmarkAlignmentLoss)","metadata":{}},{"cell_type":"code","source":"import sys\n\nsys.path.append('artifacts')","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:01:52.395713Z","iopub.execute_input":"2024-04-12T13:01:52.396039Z","iopub.status.idle":"2024-04-12T13:01:52.400832Z","shell.execute_reply.started":"2024-04-12T13:01:52.396010Z","shell.execute_reply":"2024-04-12T13:01:52.399839Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Discriminator","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch\nimport dnnlib\nimport torch_utils\nimport legacy","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:01:52.402227Z","iopub.execute_input":"2024-04-12T13:01:52.402762Z","iopub.status.idle":"2024-04-12T13:01:56.688977Z","shell.execute_reply.started":"2024-04-12T13:01:52.402732Z","shell.execute_reply":"2024-04-12T13:01:56.688065Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda')\nwith dnnlib.util.open_url('/kaggle/input/stylegan-metfaces/metfaces.pkl') as f:\n    Discriminator = legacy.load_network_pkl(f)['D'].to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:01:56.690177Z","iopub.execute_input":"2024-04-12T13:01:56.690627Z","iopub.status.idle":"2024-04-12T13:02:03.147694Z","shell.execute_reply.started":"2024-04-12T13:01:56.690595Z","shell.execute_reply":"2024-04-12T13:02:03.146900Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## ArcFace","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n__all__ = ['iresnet18', 'iresnet34', 'iresnet50', 'iresnet100', 'iresnet200']\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=3,\n                     stride=stride,\n                     padding=dilation,\n                     groups=groups,\n                     bias=False,\n                     dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=1,\n                     stride=stride,\n                     bias=False)\n\n\nclass IBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 groups=1, base_width=64, dilation=1):\n        super(IBasicBlock, self).__init__()\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        self.bn1 = nn.BatchNorm2d(inplanes, eps=1e-05,)\n        self.conv1 = conv3x3(inplanes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, eps=1e-05,)\n        self.prelu = nn.PReLU(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn3 = nn.BatchNorm2d(planes, eps=1e-05,)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n        out = self.bn1(x)\n        out = self.conv1(out)\n        out = self.bn2(out)\n        out = self.prelu(out)\n        out = self.conv2(out)\n        out = self.bn3(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        return out\n\n\nclass IResNet(nn.Module):\n    fc_scale = 7 * 7\n    def __init__(self,\n                 block, layers, dropout=0, num_features=512, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None, fp16=False):\n        super(IResNet, self).__init__()\n        self.fp16 = fp16\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes, eps=1e-05)\n        self.prelu = nn.PReLU(self.inplanes)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=2)\n        self.layer2 = self._make_layer(block,\n                                       128,\n                                       layers[1],\n                                       stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block,\n                                       256,\n                                       layers[2],\n                                       stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block,\n                                       512,\n                                       layers[3],\n                                       stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.bn2 = nn.BatchNorm2d(512 * block.expansion, eps=1e-05,)\n        self.dropout = nn.Dropout(p=dropout, inplace=True)\n        self.fc = nn.Linear(512 * block.expansion * self.fc_scale, num_features)\n        self.features = nn.BatchNorm1d(num_features, eps=1e-05)\n        nn.init.constant_(self.features.weight, 1.0)\n        self.features.weight.requires_grad = False\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, 0, 0.1)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, IBasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion, eps=1e-05, ),\n            )\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, stride, downsample, self.groups,\n                  self.base_width, previous_dilation))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(\n                block(self.inplanes,\n                      planes,\n                      groups=self.groups,\n                      base_width=self.base_width,\n                      dilation=self.dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        with torch.cuda.amp.autocast(self.fp16):\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.prelu(x)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.layer3(x)\n            x = self.layer4(x)\n            x = self.bn2(x)\n            x = torch.flatten(x, 1)\n            x = self.dropout(x)\n        x = self.fc(x.float() if self.fp16 else x)\n        x = self.features(x)\n        return x\n\n\ndef _iresnet(arch, block, layers, pretrained, progress, **kwargs):\n    model = IResNet(block, layers, **kwargs)\n    if pretrained:\n        state_dict = torch.load('/kaggle/input/arcface/backbone.pth')\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef iresnet18(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet18', IBasicBlock, [2, 2, 2, 2], pretrained,\n                    progress, **kwargs)\n\n\ndef iresnet34(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet34', IBasicBlock, [3, 4, 6, 3], pretrained,\n                    progress, **kwargs)\n\n\ndef iresnet50(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet50', IBasicBlock, [3, 4, 14, 3], pretrained,\n                    progress, **kwargs)\n\n\ndef iresnet100(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet100', IBasicBlock, [3, 13, 30, 3], pretrained,\n                    progress, **kwargs)\n\n\ndef iresnet200(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet200', IBasicBlock, [6, 26, 60, 6], pretrained,\n                    progress, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:02:03.149153Z","iopub.execute_input":"2024-04-12T13:02:03.149439Z","iopub.status.idle":"2024-04-12T13:02:03.185460Z","shell.execute_reply.started":"2024-04-12T13:02:03.149414Z","shell.execute_reply":"2024-04-12T13:02:03.184535Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"ArcFace = iresnet100(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:02:03.186491Z","iopub.execute_input":"2024-04-12T13:02:03.186748Z","iopub.status.idle":"2024-04-12T13:02:07.941728Z","shell.execute_reply.started":"2024-04-12T13:02:03.186726Z","shell.execute_reply":"2024-04-12T13:02:07.940935Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## LandmarkEstimator","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass AddCoordsTh(nn.Module):\n    def __init__(self, x_dim=64, y_dim=64, with_r=False, with_boundary=False):\n        super(AddCoordsTh, self).__init__()\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n        self.with_r = with_r\n        self.with_boundary = with_boundary\n\n    def forward(self, input_tensor, heatmap=None):\n        \"\"\"\n        input_tensor: (batch, c, x_dim, y_dim)\n        \"\"\"\n        batch_size_tensor = input_tensor.shape[0]\n\n        xx_ones = torch.ones([1, self.y_dim], dtype=torch.int32).cuda()\n        xx_ones = xx_ones.unsqueeze(-1)\n\n        xx_range = torch.arange(self.x_dim, dtype=torch.int32).unsqueeze(0).cuda()\n        xx_range = xx_range.unsqueeze(1)\n\n        xx_channel = torch.matmul(xx_ones.float(), xx_range.float())\n        xx_channel = xx_channel.unsqueeze(-1)\n\n\n        yy_ones = torch.ones([1, self.x_dim], dtype=torch.int32).cuda()\n        yy_ones = yy_ones.unsqueeze(1)\n\n        yy_range = torch.arange(self.y_dim, dtype=torch.int32).unsqueeze(0).cuda()\n        yy_range = yy_range.unsqueeze(-1)\n\n        yy_channel = torch.matmul(yy_range.float(), yy_ones.float())\n        yy_channel = yy_channel.unsqueeze(-1)\n\n        xx_channel = xx_channel.permute(0, 3, 2, 1)\n        yy_channel = yy_channel.permute(0, 3, 2, 1)\n\n        xx_channel = xx_channel / (self.x_dim - 1)\n        yy_channel = yy_channel / (self.y_dim - 1)\n\n        xx_channel = xx_channel * 2 - 1\n        yy_channel = yy_channel * 2 - 1\n\n        xx_channel = xx_channel.repeat(batch_size_tensor, 1, 1, 1)\n        yy_channel = yy_channel.repeat(batch_size_tensor, 1, 1, 1)\n\n        if self.with_boundary and type(heatmap) != type(None):\n            boundary_channel = torch.clamp(heatmap[:, -1:, :, :],\n                                        0.0, 1.0)\n\n            zero_tensor = torch.zeros_like(xx_channel)\n            xx_boundary_channel = torch.where(boundary_channel>0.05,\n                                              xx_channel, zero_tensor)\n            yy_boundary_channel = torch.where(boundary_channel>0.05,\n                                              yy_channel, zero_tensor)\n        if self.with_boundary and type(heatmap) != type(None):\n            xx_boundary_channel = xx_boundary_channel.cuda()\n            yy_boundary_channel = yy_boundary_channel.cuda()\n        ret = torch.cat([input_tensor, xx_channel, yy_channel], dim=1)\n\n\n        if self.with_r:\n            rr = torch.sqrt(torch.pow(xx_channel, 2) + torch.pow(yy_channel, 2))\n            rr = rr / torch.max(rr)\n            ret = torch.cat([ret, rr], dim=1)\n\n        if self.with_boundary and type(heatmap) != type(None):\n            ret = torch.cat([ret, xx_boundary_channel,\n                             yy_boundary_channel], dim=1)\n        return ret\n\n\nclass CoordConvTh(nn.Module):\n    \"\"\"CoordConv layer as in the paper.\"\"\"\n    def __init__(self, x_dim, y_dim, with_r, with_boundary,\n                 in_channels, first_one=False, *args, **kwargs):\n        super(CoordConvTh, self).__init__()\n        self.addcoords = AddCoordsTh(x_dim=x_dim, y_dim=y_dim, with_r=with_r,\n                                    with_boundary=with_boundary)\n        in_channels += 2\n        if with_r:\n            in_channels += 1\n        if with_boundary and not first_one:\n            in_channels += 2\n        self.conv = nn.Conv2d(in_channels=in_channels, *args, **kwargs)\n\n    def forward(self, input_tensor, heatmap=None):\n        ret = self.addcoords(input_tensor, heatmap)\n        last_channel = ret[:, -2:, :, :]\n        ret = self.conv(ret)\n        return ret, last_channel\n\n\n'''\nAn alternative implementation for PyTorch with auto-infering the x-y dimensions.\n'''\nclass AddCoords(nn.Module):\n\n    def __init__(self, with_r=False):\n        super().__init__()\n        self.with_r = with_r\n\n    def forward(self, input_tensor):\n        \"\"\"\n        Args:\n            input_tensor: shape(batch, channel, x_dim, y_dim)\n        \"\"\"\n        batch_size, _, x_dim, y_dim = input_tensor.size()\n\n        xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n        yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n\n        xx_channel = xx_channel / (x_dim - 1)\n        yy_channel = yy_channel / (y_dim - 1)\n\n        xx_channel = xx_channel * 2 - 1\n        yy_channel = yy_channel * 2 - 1\n\n        xx_channel = xx_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n        yy_channel = yy_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n\n        if input_tensor.is_cuda:\n            xx_channel = xx_channel.cuda()\n            yy_channel = yy_channel.cuda()\n\n        ret = torch.cat([\n            input_tensor,\n            xx_channel.type_as(input_tensor),\n            yy_channel.type_as(input_tensor)], dim=1)\n\n        if self.with_r:\n            rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2))\n            if input_tensor.is_cuda:\n                rr = rr.cuda()\n            ret = torch.cat([ret, rr], dim=1)\n\n        return ret\n\n\nclass CoordConv(nn.Module):\n\n    def __init__(self, in_channels, out_channels, with_r=False, **kwargs):\n        super().__init__()\n        self.addcoords = AddCoords(with_r=with_r)\n        self.conv = nn.Conv2d(in_channels + 2, out_channels, **kwargs)\n\n    def forward(self, x):\n        ret = self.addcoords(x)\n        ret = self.conv(ret)\n        return ret","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:02:07.944606Z","iopub.execute_input":"2024-04-12T13:02:07.944913Z","iopub.status.idle":"2024-04-12T13:02:07.972776Z","shell.execute_reply.started":"2024-04-12T13:02:07.944889Z","shell.execute_reply":"2024-04-12T13:02:07.971801Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n#import sys\n#sys.path.append('./faceswap/')\n#from AdaptiveWingLoss.core.coord_conv import CoordConvTh\n\n\ndef conv3x3(in_planes, out_planes, strd=1, padding=1,\n            bias=False,dilation=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3,\n                     stride=strd, padding=padding, bias=bias,\n                     dilation=dilation)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        # self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        # self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        # out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        # out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(ConvBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = conv3x3(in_planes, int(out_planes / 2))\n        self.bn2 = nn.BatchNorm2d(int(out_planes / 2))\n        self.conv2 = conv3x3(int(out_planes / 2), int(out_planes / 4),\n                             padding=1, dilation=1)\n        self.bn3 = nn.BatchNorm2d(int(out_planes / 4))\n        self.conv3 = conv3x3(int(out_planes / 4), int(out_planes / 4),\n                             padding=1, dilation=1)\n\n        if in_planes != out_planes:\n            self.downsample = nn.Sequential(\n                nn.BatchNorm2d(in_planes),\n                nn.ReLU(True),\n                nn.Conv2d(in_planes, out_planes,\n                          kernel_size=1, stride=1, bias=False),\n            )\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        residual = x\n\n        out1 = self.bn1(x)\n        out1 = F.relu(out1, True)\n        out1 = self.conv1(out1)\n\n        out2 = self.bn2(out1)\n        out2 = F.relu(out2, True)\n        out2 = self.conv2(out2)\n\n        out3 = self.bn3(out2)\n        out3 = F.relu(out3, True)\n        out3 = self.conv3(out3)\n\n        out3 = torch.cat((out1, out2, out3), 1)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        out3 += residual\n\n        return out3\n\nclass HourGlass(nn.Module):\n    def __init__(self, num_modules, depth, num_features, first_one=False):\n        super(HourGlass, self).__init__()\n        self.num_modules = num_modules\n        self.depth = depth\n        self.features = num_features\n        self.coordconv = CoordConvTh(x_dim=64, y_dim=64,\n                                     with_r=True, with_boundary=True,\n                                     in_channels=256, first_one=first_one,\n                                     out_channels=256,\n                                     kernel_size=1,\n                                     stride=1, padding=0)\n        self._generate_network(self.depth)\n\n    def _generate_network(self, level):\n        self.add_module('b1_' + str(level), ConvBlock(256, 256))\n\n        self.add_module('b2_' + str(level), ConvBlock(256, 256))\n\n        if level > 1:\n            self._generate_network(level - 1)\n        else:\n            self.add_module('b2_plus_' + str(level), ConvBlock(256, 256))\n\n        self.add_module('b3_' + str(level), ConvBlock(256, 256))\n\n    def _forward(self, level, inp):\n        # Upper branch\n        up1 = inp\n        up1 = self._modules['b1_' + str(level)](up1)\n\n        # Lower branch\n        low1 = F.avg_pool2d(inp, 2, stride=2)\n        low1 = self._modules['b2_' + str(level)](low1)\n\n        if level > 1:\n            low2 = self._forward(level - 1, low1)\n        else:\n            low2 = low1\n            low2 = self._modules['b2_plus_' + str(level)](low2)\n\n        low3 = low2\n        low3 = self._modules['b3_' + str(level)](low3)\n\n        up2 = F.upsample(low3, scale_factor=2, mode='nearest')\n\n        return up1 + up2\n\n    def forward(self, x, heatmap):\n        x, last_channel = self.coordconv(x, heatmap)\n        return self._forward(self.depth, x), last_channel\n\nclass FAN(nn.Module):\n\n    def __init__(self, num_modules=1, end_relu=False, gray_scale=False,\n                 num_landmarks=68):\n        super(FAN, self).__init__()\n        self.num_modules = num_modules\n        self.gray_scale = gray_scale\n        self.end_relu = end_relu\n        self.num_landmarks = num_landmarks\n\n        # Base part\n        if self.gray_scale:\n            self.conv1 = CoordConvTh(x_dim=256, y_dim=256,\n                                     with_r=True, with_boundary=False,\n                                     in_channels=3, out_channels=64,\n                                     kernel_size=7,\n                                     stride=2, padding=3)\n        else:\n            self.conv1 = CoordConvTh(x_dim=256, y_dim=256,\n                                     with_r=True, with_boundary=False,\n                                     in_channels=3, out_channels=64,\n                                     kernel_size=7,\n                                     stride=2, padding=3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = ConvBlock(64, 128)\n        self.conv3 = ConvBlock(128, 128)\n        self.conv4 = ConvBlock(128, 256)\n\n        # Stacking part\n        for hg_module in range(self.num_modules):\n            if hg_module == 0:\n                first_one = True\n            else:\n                first_one = False\n            self.add_module('m' + str(hg_module), HourGlass(1, 4, 256,\n                                                            first_one))\n            self.add_module('top_m_' + str(hg_module), ConvBlock(256, 256))\n            self.add_module('conv_last' + str(hg_module),\n                            nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n            self.add_module('bn_end' + str(hg_module), nn.BatchNorm2d(256))\n            self.add_module('l' + str(hg_module), nn.Conv2d(256,\n                                                            num_landmarks+1, kernel_size=1, stride=1, padding=0))\n\n            if hg_module < self.num_modules - 1:\n                self.add_module(\n                    'bl' + str(hg_module), nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n                self.add_module('al' + str(hg_module), nn.Conv2d(num_landmarks+1,\n                                                                 256, kernel_size=1, stride=1, padding=0))\n\n    def forward(self, x):\n        x, _ = self.conv1(x)\n        x = F.relu(self.bn1(x), True)\n        # x = F.relu(self.bn1(self.conv1(x)), True)\n        x = F.avg_pool2d(self.conv2(x), 2, stride=2)\n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        previous = x\n\n        outputs = []\n        boundary_channels = []\n        tmp_out = None\n        for i in range(self.num_modules):\n            hg, boundary_channel = self._modules['m' + str(i)](previous,\n                                                               tmp_out)\n\n            ll = hg\n            ll = self._modules['top_m_' + str(i)](ll)\n\n            ll = F.relu(self._modules['bn_end' + str(i)]\n                        (self._modules['conv_last' + str(i)](ll)), True)\n\n            # Predict heatmaps\n            tmp_out = self._modules['l' + str(i)](ll)\n            if self.end_relu:\n                tmp_out = F.relu(tmp_out) # HACK: Added relu\n            outputs.append(tmp_out)\n            boundary_channels.append(boundary_channel)\n\n            if i < self.num_modules - 1:\n                ll = self._modules['bl' + str(i)](ll)\n                tmp_out_ = self._modules['al' + str(i)](tmp_out)\n                previous = previous + ll + tmp_out_\n\n        return outputs, boundary_channels","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:02:07.974014Z","iopub.execute_input":"2024-04-12T13:02:07.974333Z","iopub.status.idle":"2024-04-12T13:02:08.013364Z","shell.execute_reply.started":"2024-04-12T13:02:07.974310Z","shell.execute_reply":"2024-04-12T13:02:08.012304Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"lmk = FAN(4, \"False\", \"False\", 98)\nlmk_dict = torch.load('/kaggle/input/fanmodel/WFLW_4HG.pth')\nlmk.load_state_dict(lmk_dict['state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:02:08.014446Z","iopub.execute_input":"2024-04-12T13:02:08.014722Z","iopub.status.idle":"2024-04-12T13:02:10.754816Z","shell.execute_reply.started":"2024-04-12T13:02:08.014700Z","shell.execute_reply":"2024-04-12T13:02:10.753862Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Loss Work","metadata":{}},{"cell_type":"markdown","source":"Для корректной работы FinalLoss необходимо передать следующие парамтеры:\n\n- 'fake_disc_out': выводы дискриминатора на сгенерированном изображении\n- 4 изображение: 'source', 'target', 'side', 'final'\n- 'mask_t': маска, результат encode_segmentation_rgb()","metadata":{}},{"cell_type":"code","source":"import sys\n\nsys.path.append('losses')","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:02:10.756186Z","iopub.execute_input":"2024-04-12T13:02:10.756477Z","iopub.status.idle":"2024-04-12T13:02:10.760645Z","shell.execute_reply.started":"2024-04-12T13:02:10.756453Z","shell.execute_reply":"2024-04-12T13:02:10.759630Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!pip install lpips","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:02:10.761696Z","iopub.execute_input":"2024-04-12T13:02:10.762127Z","iopub.status.idle":"2024-04-12T13:02:24.044235Z","shell.execute_reply.started":"2024-04-12T13:02:10.762071Z","shell.execute_reply":"2024-04-12T13:02:24.043064Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Collecting lpips\n  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from lpips) (2.1.2)\nRequirement already satisfied: torchvision>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from lpips) (0.16.2)\nRequirement already satisfied: numpy>=1.14.3 in /opt/conda/lib/python3.10/site-packages (from lpips) (1.26.4)\nRequirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from lpips) (1.11.4)\nRequirement already satisfied: tqdm>=4.28.1 in /opt/conda/lib/python3.10/site-packages (from lpips) (4.66.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->lpips) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.2.1->lpips) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.2.1->lpips) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.0->lpips) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.2.1->lpips) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.2.1->lpips) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.2.1->lpips) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.2.1->lpips) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\nDownloading lpips-0.1.4-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: lpips\nSuccessfully installed lpips-0.1.4\n","output_type":"stream"}]},{"cell_type":"code","source":"from GeneratorLoss import FinalLoss, AdversarialLoss, IdentityPreservationLoss\nfrom GeneratorLoss import LandmarkAlignmentLoss, ReconstructionLoss, StyleTransferLoss\n\nadversarial = AdversarialLoss()\nidentity_preservation = IdentityPreservationLoss(ArcFace=ArcFace)\nlandmark_alignment = LandmarkAlignmentLoss(LandmarkEstimator=lmk)\nreconstruction = ReconstructionLoss()\nstyle_transfer = StyleTransferLoss()\n\ngen_loss = FinalLoss(losses = [adversarial,\n                              identity_preservation,\n                              landmark_alignment,\n                              reconstruction,\n                              style_transfer],\n                    coefs = [1, 2, 0.1, 2, 0.2])","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:02:24.045876Z","iopub.execute_input":"2024-04-12T13:02:24.046303Z","iopub.status.idle":"2024-04-12T13:02:32.151273Z","shell.execute_reply.started":"2024-04-12T13:02:24.046264Z","shell.execute_reply":"2024-04-12T13:02:32.150147Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 163MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Loading model from: /opt/conda/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\n\nimg1 = Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/000001.jpg')\nimg2 = Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/000002.jpg')\nimg3 = Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/000003.jpg')\nimg4 = Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/000004.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:14:11.302558Z","iopub.execute_input":"2024-04-12T13:14:11.303158Z","iopub.status.idle":"2024-04-12T13:14:11.385492Z","shell.execute_reply.started":"2024-04-12T13:14:11.303124Z","shell.execute_reply":"2024-04-12T13:14:11.384491Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef encode_segmentation_rgb(segmentation, no_neck=True):\n    parse = segmentation[:,:,0]\n\n    face_part_ids = [1, 6, 7, 4, 5, 3, 2, 11, 12] if no_neck else [1, 6, 7, 4, 5, 3, 2, 11, 12, 17]\n    mouth_id = 10\n    hair_id = 13\n\n\n    face_map = np.zeros([parse.shape[0], parse.shape[1]])\n    mouth_map = np.zeros([parse.shape[0], parse.shape[1]])\n    hair_map = np.zeros([parse.shape[0], parse.shape[1]])\n\n    for valid_id in face_part_ids:\n        valid_index = np.where(parse==valid_id)\n        face_map[valid_index] = 255\n    valid_index = np.where(parse==mouth_id)\n    mouth_map[valid_index] = 255\n    valid_index = np.where(parse==hair_id)\n    hair_map[valid_index] = 255\n\n    return np.stack([face_map, mouth_map, hair_map], axis=2)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:36:43.705580Z","iopub.execute_input":"2024-04-12T13:36:43.706206Z","iopub.status.idle":"2024-04-12T13:36:43.714433Z","shell.execute_reply.started":"2024-04-12T13:36:43.706176Z","shell.execute_reply":"2024-04-12T13:36:43.713506Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import torchvision\n\nmask_t = encode_segmentation_rgb(torchvision.transforms.functional.pil_to_tensor(img2))","metadata":{"execution":{"iopub.status.busy":"2024-04-12T13:36:46.432973Z","iopub.execute_input":"2024-04-12T13:36:46.433802Z","iopub.status.idle":"2024-04-12T13:36:46.445328Z","shell.execute_reply.started":"2024-04-12T13:36:46.433768Z","shell.execute_reply":"2024-04-12T13:36:46.444542Z"},"trusted":true},"execution_count":38,"outputs":[]}]}